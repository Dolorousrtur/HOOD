{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d96a4911-2cfc-4234-9098-453018ec4436",
   "metadata": {},
   "source": [
    "# Inference from any pose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bd975a-3960-43ce-8ede-2ac20fb3af2c",
   "metadata": {},
   "source": [
    "With this notebook you can run inference of HOOD starting with a garment in any pose. For the body sequence you can use either:\n",
    "* Sequence of SMPL parameters (see `$HOOD_DATA/fromanypose/pose_sequence.pkl` as an example) or\n",
    "* Sequence of meshes (see `$HOOD_DATA/fromanypose/mesh_sequence.pkl` as an example)\n",
    "\n",
    "You also need to have a garment mesh (as `.obj` file) which is aligned with the first frame of your body sequence.\n",
    "\n",
    "In this notebook we first show how to convert a garment mesh in `.obj` format into a `.pkl` template used in HOOD. \n",
    "\n",
    "Then, we show how to use the configuration file `aux/from_any_pose.yaml` to run inference with over albitrary SMPL sequence and arbitrary mesh sequence.\n",
    "\n",
    "Note that, pinned vertices are not yet supported in this example, so do not expect it to work with lower body garments (they would slide down :c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b141c5-3f89-4d81-90ae-566296bd6e23",
   "metadata": {},
   "source": [
    "## Set environmental variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec571b31-88dc-4d7d-b975-d0c8b8eae87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HOOD_PROJECT =  \"/path/to/hood/repository\"\n",
    "HOOD_DATA = \"/path/to/hood/data\"\n",
    "\n",
    "os.environ[\"HOOD_PROJECT\"] = HOOD_PROJECT\n",
    "os.environ[\"HOOD_DATA\"] = HOOD_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a91eba6-68a4-401b-bbfe-33292ca5ba4b",
   "metadata": {},
   "source": [
    "## Create template file from the .obj file:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b41a41-90b3-48b5-8447-858797c9dd53",
   "metadata": {},
   "source": [
    "Use `utils.mesh_creation::obj2template()` function to convert an `.obj` file into a template dictionary and then save it with `pickle_dump`\n",
    "\n",
    "`$HOOD_DATA/fromanypose/tshirt.obj` is provided as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0568c817-32a6-4dfd-9ece-b5005dcc800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "from utils.mesh_creation import obj2template\n",
    "from utils.common import pickle_load, pickle_dump\n",
    "from utils.defaults import DEFAULTS\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a972f1e0-af66-4b8a-9b4e-f0ab41807a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj_path = Path(DEFAULTS.data_root) / 'fromanypose' / 'tshirt.obj'\n",
    "# out_template_path = Path(DEFAULTS.data_root) / 'fromanypose' / 'tshirt.pkl'\n",
    "\n",
    "obj_path = Path(DEFAULTS.data_root) / 'fromanypose' / 'skirt.obj'\n",
    "out_template_path = Path(DEFAULTS.data_root) / 'fromanypose' / 'skirt.pkl'\n",
    "\n",
    "template_dict = obj2template(obj_path, verbose=True)\n",
    "pickle_dump(template_dict, out_template_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8536a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mesh_creation import add_pinned_verts, add_pinned_verts_single_template\n",
    "\n",
    "\n",
    "pinned_indices = \\\n",
    "[2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215,\n",
    " 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235,\n",
    " 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2327, 2328, 2351, 2492, 2497, 2669, 2670, 2671, 2674, 2837, 3139,\n",
    " 3200, 3204, 3359, 3362, 3495, 3512, 3634, 3638, 3805, 3965, 3967, 4133, 4137, 4140, 4335, 4340, 4506, 4509, 4669, 4674,\n",
    " 4749, 4812, 4849, 4853, 5138, 5309, 5342, 5469, 5474, 5503, 5646, 5650, 5654, 5855, 5857, 6028, 6091, 6204, 6209, 6280,\n",
    " 6374, 6377, 6378, 6473, 6649, 6654, 6814, 6817, 6986, 6989, 6990, 6992, 7172, 7178, 7336, 7500, 7505, 7661, 7665, 7666]\n",
    "\n",
    "add_pinned_verts_single_template(out_template_path, pinned_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ec39d2-8dd8-4161-bec6-7a6390ba1ff4",
   "metadata": {},
   "source": [
    "## Inference with a SMPL sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c84b2f1-f186-4559-ae37-b891f2f4b664",
   "metadata": {},
   "source": [
    "Here we use a sequence of SMPL parameters as a body sequence. The garment template needs to be aligned with the first frame of the sequence.\n",
    "\n",
    "The SMPL pose sequence has to be a `.pkl` file containing a dictionary of the following items (see `$HOOD_DATA/fromanypose/pose_sequence.pkl` as an example):\n",
    "* `body_pose`: np.array [N, 69]\n",
    "* `global_orient`: np.array [N, 3]\n",
    "* `transl`: np.array [N, 3]\n",
    "* `betas`: np.array [10,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f760ca04-f5f7-4543-89bc-9a46a01896eb",
   "metadata": {},
   "source": [
    "### Load runner from `from_any_pose` config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f676f90-1e9a-446a-85c8-7ccf61fbc53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.validation import Config as ValidationConfig\n",
    "from utils.arguments import load_params, create_modules\n",
    "from utils.arguments import load_params\n",
    "from utils.common import move2device, pickle_dump\n",
    "from utils.defaults import DEFAULTS\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from utils.arguments import create_runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dc5a9d-6d23-4e70-afff-2530c7b07332",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules, config = load_params('aux/from_any_pose')\n",
    "\n",
    "runner_module, runner, aux_modules = create_runner(modules, config)\n",
    "\n",
    "checkpoint_path = Path(DEFAULTS.data_root) / 'trained_models' / 'postcvpr.pth'\n",
    "state_dict =  torch.load(checkpoint_path)\n",
    "runner.load_state_dict(state_dict['training_module'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdef2805",
   "metadata": {},
   "source": [
    "### Create a data loader\n",
    "\n",
    "To do that, first, set several variables to specify which garment and which pose sequence you want to use:\n",
    "\n",
    "- `pose_sequence_type`: type of the pose sequence can be either `smpl` or `mesh`. For this example, set `smpl`\n",
    "- `pose_sequence_path` path to the smpl pose sequence relative to `$HOOD_DATA`. For this example we use `fromanypose/pose_sequence.pkl`\n",
    "- `garment_template_path` path to the .pkl file with the garment template  For this example we use `fromanypose/tshirt.pkl`\n",
    "- `smpl_model` path to the SMPL model you want to use relative to `$HOOD_DATA`.  For this example we use `smpl/SMPL_FEMALE.pkl`\n",
    "\n",
    "\n",
    "Then, we create a dataloader with `make_fromanypose_dataloader()` function (see its code for more details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fccef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import make_fromanypose_dataloader\n",
    "\n",
    "pose_sequence_type = 'smpl'\n",
    "pose_sequence_path = Path('fromanypose') / 'pose_sequence.pkl'\n",
    "# garment_template_path = Path('fromanypose') / 'tshirt.pkl'\n",
    "garment_template_path = Path('fromanypose') / 'skirt.pkl'\n",
    "smpl_model = 'smpl/SMPL_FEMALE.pkl'\n",
    "\n",
    "dataloader = make_fromanypose_dataloader(pose_sequence_type=pose_sequence_type, \n",
    "                       pose_sequence_path=pose_sequence_path, \n",
    "                       garment_template_path=garment_template_path, \n",
    "                       smpl_model=smpl_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c952eac2-864b-4a4f-b20b-13bcaafccd26",
   "metadata": {},
   "source": [
    "### load sample, infer and save trajectories\n",
    "\n",
    "To visualise saved trajectories, see `Inference.ipynb::write_video`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b411d7f2-3d83-431d-b8a3-08937646f5bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = next(iter(dataloader))\n",
    "\n",
    "trajectories_dict = runner.valid_rollout(sample)\n",
    "# Save the sequence to disc\n",
    "out_path = Path(DEFAULTS.data_root) / 'temp' / 'output_f.pkl'\n",
    "print(f\"Rollout saved into {out_path}\")\n",
    "pickle_dump(dict(trajectories_dict), out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc63433-3d5a-4583-9636-1562ea1f62f2",
   "metadata": {},
   "source": [
    "## Inference with a mesh sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffabf78b-9afb-409a-8cda-3433e9b93308",
   "metadata": {},
   "source": [
    "Here we use a sequence of arbitrary meshes.  The garment template needs to be aligned with the first frame of the sequence.\n",
    "\n",
    "The mesh sequence has to be a `.pkl` file containing a dictionary of the following items (see `$HOOD_DATA/fromanypose/mesh_sequence.pkl` as an example):\n",
    "* `verts`: np.array [N, 3]\n",
    "* `faces`: np.array [F, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facedbd7-2d8b-4131-bcc9-a9c688c5b5ed",
   "metadata": {},
   "source": [
    "### Load runner and dataloader from `from_any_pose` config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a143d7ec-7e0e-4765-86b4-463eaed6baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.validation import Config as ValidationConfig\n",
    "from utils.arguments import load_params, create_modules\n",
    "from utils.arguments import load_params\n",
    "from utils.common import move2device, pickle_dump\n",
    "from utils.defaults import DEFAULTS\n",
    "from pathlib import Path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07db298-afb1-4b72-af26-7842a6947626",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules, config = load_params('aux/from_any_pose')\n",
    "\n",
    "\n",
    "runner_module, runner, aux_modules = create_runner(modules, config)\n",
    "\n",
    "checkpoint_path = Path(DEFAULTS.data_root) / 'trained_models' / 'postcvpr.pth'\n",
    "state_dict =  torch.load(checkpoint_path)\n",
    "runner.load_state_dict(state_dict['training_module'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56133fff",
   "metadata": {},
   "source": [
    "### Create a data loader\n",
    "\n",
    "Same as in the previous part, set several variables to specify which garment and which pose sequence you want to use:\n",
    "\n",
    "- `pose_sequence_type`: type of the pose sequence can be either `smpl` or `mesh`. For this example, set `mesh`\n",
    "- `pose_sequence_path` path to the smpl pose sequence relative to `$HOOD_DATA`. For this example we use `fromanypose/mesh_sequence.pkl`\n",
    "- `garment_template_path` path to the .pkl file with the garment template  For this example we use `fromanypose/tshirt.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae7ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_sequence_type = 'mesh'\n",
    "pose_sequence_path = Path('fromanypose') / 'mesh_sequence.pkl'\n",
    "# garment_template_path = Path('fromanypose') / 'tshirt.pkl'\n",
    "garment_template_path = Path('fromanypose') / 'skirt.pkl'\n",
    "\n",
    "\n",
    "dataloader = make_fromanypose_dataloader(pose_sequence_type=pose_sequence_type, \n",
    "                       pose_sequence_path=pose_sequence_path, \n",
    "                       garment_template_path=garment_template_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a43ad4-5bd9-4ae9-939b-f804dea17dd0",
   "metadata": {},
   "source": [
    "### load sample, infer and save trajectories\n",
    "\n",
    "To visualise saved trajectories, see `Inference.ipynb::write_video`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d81b569-69d4-4989-9035-6127d39e26b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(dataloader))\n",
    "\n",
    "trajectories_dict = runner.valid_rollout(sample)\n",
    "# Save the sequence to disc\n",
    "out_path = Path(DEFAULTS.data_root) / 'temp' / 'output_m.pkl'\n",
    "print(f\"Rollout saved into {out_path}\")\n",
    "pickle_dump(dict(trajectories_dict), out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
