{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "413e290d-dedd-48c3-893b-c5641ce9f2b7",
   "metadata": {},
   "source": [
    "# set enviromental variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1ee6e6-0226-4b7b-bbe7-3a4b39a7e85c",
   "metadata": {},
   "source": [
    "Make sure you set these two enviromental variables:\n",
    "\n",
    "* `HOOD_PROJECT` should lead to the HOOD repository\n",
    "* `HOOD_DATA` should lead to the data folder (see `README.md` for details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6008d13f-9825-4b9e-866c-910bfd4a6ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HOOD_PROJECT\"] = \"/path/to/hood/repository\"\n",
    "os.environ[\"HOOD_DATA\"] = \"/path/to/hood/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3bbf5a-b460-4d0e-bdac-1d03248b4b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOOD_PROJECT = os.environ[\"HOOD_PROJECT\"]\n",
    "HOOD_DATA = os.environ[\"HOOD_DATA\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52065c8-4491-41a9-acb5-195879c78a81",
   "metadata": {},
   "source": [
    "# Prepare pose sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ad82e6-635c-432a-9377-33bbc4bc7ce6",
   "metadata": {},
   "source": [
    "To infer the HOOD models over a specific sequence of body poses, you first need to convert the pose sequence into a `.pkl` file:\n",
    "\n",
    "The `.pkl` file should contain a dictionary with the following contents\n",
    "* `body_pose`: numpy array of shape \\[Nx69\\] with a sequence of SMPL pose parameters\n",
    "* `global_orient`: global orientations of the body, \\[Nx3\\] array\n",
    "* `transl`: global translations of the body, \\[Nx3\\] array\n",
    "* `betas`: SMPL shape parameters, \\[10,\\] array\n",
    "\n",
    "\n",
    "Here we provide the functions to generate such files from two sources:\n",
    "* VTO dataset (clone [this repository](https://github.com/isantesteban/vto-dataset) to download, all data is in the repo)\n",
    "* AMASS dataset (download **CMU** split with **SMPL+H** parameters from [here](https://amass.is.tue.mpg.de/))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2c22b8-7eaf-4ba9-a425-7ded18c1c275",
   "metadata": {},
   "source": [
    "## sequence from the VTO dataset &rarr;  HOOD .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49060da0-7d1a-49ae-80f7-5d5b8049ce04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from utils.data_making import convert_vto_to_pkl\n",
    "\n",
    "\n",
    "VTO_DATASET_PATH = '/path/to/vto-dataset/'\n",
    "\n",
    "vto_sequence_path = Path(VTO_DATASET_PATH) / 'tshirt/simulations/tshirt_shape00_01_01.pkl'\n",
    "target_pkl_path =  Path(HOOD_DATA) / 'temp/01_01.pkl'\n",
    "\n",
    "\n",
    "convert_vto_to_pkl(vto_sequence_path, target_pkl_path, n_zeropose_interpolation_steps=30)\n",
    "print(f'Pose sequence saved into {target_pkl_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada80264-697f-40f2-9edf-65e434b9f984",
   "metadata": {},
   "source": [
    "## AMASS .npz sequences &rarr;  HOOD .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2336769-74be-454e-b0f0-0558704335f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_making import convert_amass_to_pkl\n",
    "\n",
    "AMASS_DATASET_PATH = '/path/to/AMASS/'\n",
    "\n",
    "amass_seq_path = Path(AMASS_DATASET_PATH) / 'CMU/01/01_01_poses.npz'\n",
    "target_pkl_path =  Path(HOOD_DATA) / 'temp/01_01.pkl'\n",
    "\n",
    "convert_amass_to_pkl(amass_seq_path, target_pkl_path, target_fps=30)\n",
    "print(f'Pose sequence saved into {target_pkl_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa3ba9-6b0f-4457-afb2-03016cadc4d5",
   "metadata": {},
   "source": [
    "# Choose a garment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22942f52-5854-4bdd-8d98-c0d06bf0a281",
   "metadata": {},
   "source": [
    "Next, you need to choose a garment to simulate.\n",
    "\n",
    "Its template and some auxiliary data should be stored in the `$HOOD_DATA/aux_data/garments_dict.pkl` file\n",
    "\n",
    "You can choose from the list of garments already provided in this file:\n",
    "\n",
    "![all_garments](static/all_garments.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69656be-c3d7-40e9-8605-fa79fb8ebdf1",
   "metadata": {},
   "source": [
    "Or you can import a new garment from an `.obj` file\n",
    "\n",
    "We also provide `.obj` files for all garments usen in the paper in `$HOOD_DATA/aux_data/garment_meshes/` directory.\n",
    "Note that these `.obj` files only have demonstrational purpose. \n",
    "For inference and training we use garment data stored in  `$HOOD_DATA/aux_data/garments_dict.pkl`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c2e112-63eb-4484-a0b7-8678ac2a3630",
   "metadata": {},
   "source": [
    "## Add your own garment from an `.obj` file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f6ed06-b020-48fe-8e32-034957110def",
   "metadata": {},
   "source": [
    "First, add the garment to the `garments_dict.pkl` file using `add_garment_to_garments_dict` function.\n",
    "\n",
    "It builds a dictionary for the garment that contains:\n",
    "* `rest_pos`: \\[Nx3\\], positions of the vertices in canonical pose that are aligned to zero- pose and shape SMPL body.\n",
    "* `faces`: \\[Fx3\\], triplets of node indices that constitute each face\n",
    "* `node_type` \\[Nx1\\], node type labels (`0` for regular, `3` for \"pinned\"). By default, all nodes are regular, we show how to add \"pinned nodes\" later in this notebook\n",
    "* `lbs` dictionary with shape- and pose- blendshapes and skinning weights for the garment, sampled from SMPL model\n",
    "* `center` and `coarse_edges` info on long-range (coarse) edges used to build hiererchical graph of the garment.\n",
    "\n",
    "To be able to start simulation from arbitrary pose, we use linear blend-skinning (LBS) to initialize the garment geometry in the first frame. For each garment node we sample pose-and shape-blendshapes and skinning weights from the closest SMPL node in canonical pose.\n",
    "\n",
    "However, for loose garments such approach may result in overly-stretched triangles. Therefore, we use the approach introduced in [\\[Santesteban et al. 2021\\]](http://mslab.es/projects/SelfSupervisedGarmentCollisions/) and average skinning weights and blendshapes over many randomly sampled SMPL nodes around the given garment node.\n",
    "\n",
    "The parameter `n_samples_lbs` controls the number of random samples to use. We recommend setting it to 0 for tight-fitting garments (shirts, pants) and to 1000 for loose ones (skirts, dresses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468c0e51-2d23-46e2-a789-e7757c113292",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.mesh_creation import add_garment_to_garments_dict, add_pinned_verts\n",
    "from utils.defaults import DEFAULTS\n",
    "\n",
    "garment_obj_path = os.path.join(DEFAULTS.aux_data, 'garment_meshes', 'longskirt.obj')\n",
    "smpl_file = os.path.join(DEFAULTS.aux_data, 'smpl', 'SMPL_FEMALE.pkl')\n",
    "garments_dict_path = os.path.join(DEFAULTS.aux_data, 'garments_dict.pkl')\n",
    "\n",
    "# Name of the garment we are adding\n",
    "garment_name = 'longskirt'\n",
    "\n",
    "add_garment_to_garments_dict(garment_obj_path, garments_dict_path, garment_name, smpl_file=smpl_file, n_samples_lbs=1000, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff85fb86-598b-4b48-baa8-903a89b84636",
   "metadata": {},
   "source": [
    "### Add pinned vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e9df6b-44c8-4c7c-a3d3-3b60d0e1e4bf",
   "metadata": {},
   "source": [
    "For some gaments, it can be necessary to fix positions of a subset of nodes relative to the body. For example, fix the top ring of a skirt or pants to prevent it from falling off the body.\n",
    "\n",
    "To label a set of garment nodes as \"pinned\", you need to use `add_pinned_verts` function and provide it with the list of node indices that you want to pin.\n",
    "\n",
    "One easy way of getting indices for a set of nodes, is by using [Blender](https://www.blender.org/). \n",
    "\n",
    "1. Open it, import the garment from the `.obj` file. \n",
    "2. Then in `Layout` tab press `Tab` to go into the editing mode. \n",
    "3. Select all vertices you want to pin. \n",
    "4. Then, go into `Scripting` tab and execute the following piece of code there.\n",
    "\n",
    "```python\n",
    "import bpy\n",
    "import bmesh\n",
    "\n",
    "obj = bpy.context.active_object\n",
    "bm = bmesh.from_edit_mesh(obj.data)    \n",
    "obj = bpy.context.active_object; bm = bmesh.from_edit_mesh(obj.data)    ; selected = [i.index for i in bm.verts if i.select == True]; print(selected)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "5. You will get a list of indices for the selected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f4d502-920d-44b2-bcf1-ca0814057c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinned_indices = \\\n",
    "[2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215,\n",
    " 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235,\n",
    " 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2327, 2328, 2351, 2492, 2497, 2669, 2670, 2671, 2674, 2837, 3139,\n",
    " 3200, 3204, 3359, 3362, 3495, 3512, 3634, 3638, 3805, 3965, 3967, 4133, 4137, 4140, 4335, 4340, 4506, 4509, 4669, 4674,\n",
    " 4749, 4812, 4849, 4853, 5138, 5309, 5342, 5469, 5474, 5503, 5646, 5650, 5654, 5855, 5857, 6028, 6091, 6204, 6209, 6280,\n",
    " 6374, 6377, 6378, 6473, 6649, 6654, 6814, 6817, 6986, 6989, 6990, 6992, 7172, 7178, 7336, 7500, 7505, 7661, 7665, 7666]\n",
    "\n",
    "add_pinned_verts(garments_dict_path, garment_name, pinned_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab918a1-0c18-4e0c-a19f-dadd9eb9b44b",
   "metadata": {},
   "source": [
    "# Generate rollout for one sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ef67ff-a467-4b3b-820c-83ab53342fb8",
   "metadata": {},
   "source": [
    "Once we created a `.pkl` file wth a pose sequence and added our garment to the `garments_dict.pkl` (or you can use one of the garments that are already there), we can generate a rollout sequence for them using a trained HOOD model.\n",
    "\n",
    "We provide 4 pretrained models and corresponding configuration files for each of them. The weights of the trained models are located in `$HOOD_DATA/trained_models`. The configuration files are in  `$HOOD_PROJECT/configs`\n",
    "\n",
    "| model file      | config name           | comments                                                                                                                                                                                                                            |\n",
    "|-----------------|-----------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| fine15          | cvpr_baselines/fine15 | Baseline model denoted as `Fine15` in the paper. No long-range edges, 15 message-passing steps.                                                                                                                                     |\n",
    "| fine18          | cvpr_baselines/fine48 | Baseline model denoted as `Fine48` in the paper. No long-range edges, 48 message-passing steps.                                                                                                                                     |\n",
    "| cvpr_submission | cvpr                  | Model used in the CVPR paper. Use it if you want to compare to the paper.                                                                                                                                                           |\n",
    "| postcvpr        | postcvpr              | Newer model trained using refactored code with minor bug fixes. Use it if you want to use HOOD model in a downstream task.  Also use this config if you want to train a HOOD model from scratch (see `Training.ipynb` for details) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366caa43-f3e1-4ff2-9f4f-02087a09fc80",
   "metadata": {},
   "source": [
    "### create validation config and create `Runner` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3eb786-1682-4894-995a-366dd7c92e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.validation import Config as ValidationConfig\n",
    "from utils.validation import load_runner_from_checkpoint, update_config_for_validation, create_one_sequence_dataloader\n",
    "from utils.arguments import load_params\n",
    "from utils.common import move2device, pickle_dump\n",
    "from utils.defaults import DEFAULTS\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Set material paramenters, see configs/cvpr.yaml for the training ranges for each parameter\n",
    "config_dict = dict()\n",
    "config_dict['density'] = 0.20022\n",
    "config_dict['lame_mu'] = 23600.0\n",
    "config_dict['lame_lambda'] = 44400\n",
    "config_dict['bending_coeff'] = 3.962e-05\n",
    "\n",
    "# If True, the SMPL poses are slightly modified to avoid hand-body self-penetrations. The technique is adopted from the code of SNUG \n",
    "config_dict['separate_arms'] = True\n",
    "\n",
    "# Paths to SMPL model and garments_dict file relative to $HOOD_DATA/aux_data\n",
    "config_dict['garment_dict_file'] = 'garments_dict.pkl'\n",
    "config_dict['smpl_model'] = 'smpl/SMPL_FEMALE.pkl'\n",
    "\n",
    "validation_config = ValidationConfig(**config_dict)\n",
    "\n",
    "\n",
    "# Choose the model and the configuration file\n",
    "\n",
    "# config_name = 'cvpr_baselines/fine15'\n",
    "# checkpoint_path = Path(DEFAULTS.data_root) / 'trained_models' / 'fine15.pth'\n",
    "\n",
    "# config_name = 'cvpr_baselines/fine48'\n",
    "# checkpoint_path = Path(DEFAULTS.data_root) / 'trained_models' / 'fine48.pth'\n",
    "\n",
    "# config_name = 'cvpr'\n",
    "# checkpoint_path = Path(DEFAULTS.data_root) / 'trained_models' / 'cvpr_submission.pth'\n",
    "\n",
    "config_name = 'postcvpr'\n",
    "checkpoint_path = Path(DEFAULTS.data_root) / 'trained_models' / 'postcvpr.pth'\n",
    "\n",
    "\n",
    "garments_dict_path = os.path.join(DEFAULTS.aux_data, 'garments_dict.pkl')\n",
    "\n",
    "# load the config from .yaml file and load .py modules specified there\n",
    "modules, experiment_config = load_params(config_name)\n",
    "\n",
    "# modify the config to use it in validation \n",
    "experiment_config = update_config_for_validation(experiment_config, validation_config)\n",
    "\n",
    "# load Runner object and the .py module it is declared in\n",
    "runner_module, runner = load_runner_from_checkpoint(checkpoint_path, modules, experiment_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d6421-9e94-41a9-83fb-6373ef431e18",
   "metadata": {},
   "source": [
    "### create one-sequence dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e00cfc-0602-4109-9817-5d0690ab0778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file with the pose sequence\n",
    "sequence_path =  Path(HOOD_DATA) / 'temp/01_01.pkl'\n",
    "\n",
    "\n",
    "# name of the garment to sumulate\n",
    "garment_name = 'longskirt'\n",
    "\n",
    "dataloader = create_one_sequence_dataloader(sequence_path, garment_name, modules, experiment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d952c3-0def-410e-b152-e9e3dd99e11a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequence = next(iter(dataloader))\n",
    "sequence = move2device(sequence, 'cuda:0')\n",
    "trajectories_dict = runner.valid_rollout(sequence,  bare=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3476f3-3b26-4224-b27a-4cbfc4ed0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sequence to disc\n",
    "out_path = Path(DEFAULTS.data_root) / 'temp' / 'output.pkl'\n",
    "print(f\"Rollout saved into {out_path}\")\n",
    "pickle_dump(dict(trajectories_dict), out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733d150e-aa88-420a-b22c-02c561f205aa",
   "metadata": {},
   "source": [
    "### write video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64485f0f-55ef-45e2-b64a-9ef2e5fc27c6",
   "metadata": {},
   "source": [
    "Finally, we can render a video of the generated sequence with [aitviewer](https://github.com/eth-ait/aitviewer)\n",
    "\n",
    "Or you can render it interactively using `python utils/show.py rollout_path=PATH_TO_SEQUENCE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d616f-f90f-4c19-bc9a-6980da5718e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.show import write_video \n",
    "from aitviewer.headless import HeadlessRenderer\n",
    "\n",
    "# Careful!: creating more that one renderer in a single session causes an error\n",
    "renderer = HeadlessRenderer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c43b212-2ec8-4459-9442-a1dae7eb349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = Path(DEFAULTS.data_root) / 'temp' / 'output.pkl'\n",
    "out_video = Path(DEFAULTS.data_root) / 'temp' / 'output.mp4'\n",
    "write_video(out_path, out_video, renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2434c581-d469-415a-a046-1b78370f8e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hood",
   "language": "python",
   "name": "hood"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
